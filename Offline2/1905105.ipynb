{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1: Load the Telco data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the churn dataset\n",
    "def Load_telco():\n",
    "    data = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    # Preview the data\n",
    "    # data.head(10)\n",
    "    data.drop('customerID', axis=1, inplace=True) # Drop the customerID column\n",
    "    Features = data.drop('Churn', axis=1) # Features\n",
    "    Labels = data['Churn'] # Labels\n",
    "    map_Labels = {'Yes': 1, 'No': 0} # Map the labels to 0 and 1\n",
    "    Labels = [map_Labels[i] for i in Labels] # Replace the labels with 0 and 1\n",
    "    Labels = pd.Series(Labels) # Convert the labels to a pandas series\n",
    "    Features['TotalCharges'] = pd.to_numeric(Features['TotalCharges'], errors='coerce') # Convert TotalCharges to numeric\n",
    "    Features['MultipleLines'] = Features['MultipleLines'].replace('No phone service', 'No') # Replace 'No phone service' with 'No'\n",
    "    # Features.isnull().sum() # Count the number of missing values in each column\n",
    "    Features.fillna(Features.mean(numeric_only=True), inplace=True) # Fill the missing values with the mean of the column\n",
    "    # Features\n",
    "    categorical_columns = Features.select_dtypes(include=['object']).columns # Select the categorical columns\n",
    "    for column in categorical_columns:\n",
    "        Features[column] = Features[column].astype('category')\n",
    "    Features = pd.get_dummies(Features) # One-hot encode the categorical columns\n",
    "    scaler = MinMaxScaler() # Create a MinMaxScaler object\n",
    "    Features = pd.DataFrame(scaler.fit_transform(Features), columns=Features.columns) # Normalize the features\n",
    "    Labels_array = Labels.to_numpy() # Convert the labels to a numpy array\n",
    "    return Features, Labels_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1: Load the Adult data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the adult.data file\n",
    "def Load_adult():\n",
    "    train = pd.read_csv('adult/adult.data', header=None, skipinitialspace=True)\n",
    "    test = pd.read_csv('adult/adult.test', header=None, skipinitialspace=True, skiprows=1)\n",
    "    print(train.shape)\n",
    "    print(test.shape)\n",
    "    # un = test['income'].unique() \n",
    "    # print(un)\n",
    "    data = pd.concat([train, test])\n",
    "    data.columns =  ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                        'marital-status', 'occupation', 'relationship', 'race',\n",
    "                        'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                        'native-country', 'income']\n",
    "    # for column in data.columns:\n",
    "    #     if data[column].dtype == 'object':\n",
    "    #         print (column)\n",
    "    #         print(data[column].unique()) # Print the unique values in the column.\n",
    "    #         There are missing values in the dataset. The missing values are represented by a question mark '?' in workclass, occupation and native-country columns.\n",
    "    data = data.replace('?', np.nan) # Replace '?' with np.nan\n",
    "    data = data.apply(lambda x: x.str.rstrip('.') if x.dtype == \"object\" else x) # Remove the '.' at the end of the values in the columns\n",
    "    # data\n",
    "    Features = data.drop('income', axis=1) # Features\n",
    "    Labels = data['income'] # Labels\n",
    "    map_Labels = {'>50K': 1, '<=50K': 0} # Map the labels to 0 and 1\n",
    "    Labels = [map_Labels[i] for i in Labels] # Replace the labels with 0 and 1\n",
    "    # Features.isnull().sum() # Count the number of missing values in each column\n",
    "    Features.fillna(Features.mode().iloc[0], inplace=True) # Fill the missing values with the mode of the column\n",
    "    # Features.isnull().sum() # Count the number of missing values in each column\n",
    "    categorical_columns = Features.select_dtypes(include=['object']).columns # Select the categorical columns\n",
    "    for column in categorical_columns:\n",
    "        Features[column] = Features[column].astype('category')\n",
    "    Features = pd.get_dummies(Features) # One-hot encode the categorical columns\n",
    "    scaler = MinMaxScaler() # Create a MinMaxScaler object\n",
    "    Features = pd.DataFrame(scaler.fit_transform(Features), columns=Features.columns) # Normalize the features  \n",
    "    # Features\n",
    "    Labels_array = np.array(Labels) # Convert the labels to a numpy array\n",
    "    # Labels_array\n",
    "    return Features, Labels_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1: Load the CreditCard data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the creditcard.csv file\n",
    "def Load_creditcard():\n",
    "    data = pd.read_csv('creditcard.csv')\n",
    "    # data['Class'].value_counts()\n",
    "    # data\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    frauds = data[data['Class'] == 1]\n",
    "    not_frauds = data[data['Class'] == 0]\n",
    "    not_frauds = not_frauds.sample(n=20000, random_state=42)\n",
    "    data = pd.concat([frauds, not_frauds])\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    Features = data.drop('Class', axis=1) # Features\n",
    "    Labels = data['Class'] # Labels\n",
    "    scaler = MinMaxScaler() # Create a MinMaxScaler object\n",
    "    Features = pd.DataFrame(scaler.fit_transform(Features), columns=Features.columns) # Normalize the features\n",
    "    Labels_array = np.array(Labels) # Convert the labels to a numpy array\n",
    "    return Features, Labels_array\n",
    "    # Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Logistic Regression Class</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_params(n_features):\n",
    "        weights = np.zeros(n_features) # Initialize weights to zeros n_features is the number of features\n",
    "        bias = 0\n",
    "        return weights, bias # rerturn the initial weights and bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_loss(y, y_pred):\n",
    "        n_samples = y.shape[0]\n",
    "        loss = -(1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred)) # Compute the loss\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_descent(X, y, weights, bias, learning_rate, n_iters):\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            # Linear model: z = X * weights + bias\n",
    "            z = np.dot(X, weights) + bias\n",
    "            y_pred = LogisticRegression.sigmoid(z)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            # print(\"iteration:\", i) # Print the iteration    \n",
    "            # print(LogisticRegression.compute_loss(y, y_pred)) # Print the loss\n",
    "            # Update weights and bias\n",
    "            weights -= learning_rate * dw\n",
    "            bias -= learning_rate * db\n",
    "            \n",
    "        return weights, bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit(X, y, learning_rate=0.001, n_iters=1000):\n",
    "        n_features = X.shape[1] # Taking only the number of columns\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        weights, bias = LogisticRegression.initialize_params(n_features)\n",
    "        \n",
    "        # Perform gradient descent\n",
    "        weights, bias = LogisticRegression.gradient_descent(X, y, weights, bias, learning_rate, n_iters)\n",
    "        \n",
    "        return weights, bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(X, weights, bias):\n",
    "        # Linear model and applying sigmoid to get probabilities\n",
    "        z = np.dot(X, weights) + bias\n",
    "        y_pred = LogisticRegression.sigmoid(z)\n",
    "        \n",
    "        # Convert probabilities to binary class (0 or 1)\n",
    "        return [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Splitting the dataset into Train, Validation & Test Dataset </b>\n",
    "Here for 3 datasets, we have to comment the 2 datasets not needed and uncomment the target dataset in 2,3 & 4 number lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 15)\n",
      "(16281, 15)\n",
      "(31258, 105) (31258,)\n",
      "(7815, 105) (7815,)\n",
      "(9769, 105) (9769,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Features, Labels_array = Load_telco()\n",
    "Features, Labels_array = Load_adult()\n",
    "# Features, Labels_array = Load_creditcard()\n",
    "\n",
    "# Split the data into training+validation and testing sets\n",
    "X_tr_val, X_test, y_tr_val, y_test = train_test_split(Features, Labels_array, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_tr_val = Features[:32561]\n",
    "# X_test = Features[32561:]\n",
    "# y_tr_val = Labels_array[:32561]\n",
    "# y_test = Labels_array[Labels_array[32561:]]\n",
    "# Split the training+validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr_val, y_tr_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Training the model\n",
    "# weights, bias = LogisticRegression.fit(X_train, y_train, learning_rate=0.95, n_iters=1000) # lr = 0.95 (creditcard), lr = 0.8 (adult), lr = 0.5 (telco)\n",
    "\n",
    "# # Predicting\n",
    "# predictions = LogisticRegression.predict(X_test, weights, bias)\n",
    "\n",
    "# print(\"Predictions:\", predictions)\n",
    "# # Evaluate the model\n",
    "# accuracy = np.mean(predictions == y_test)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "# print(np.unique(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Bagging </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bagging(X_train, y_train):\n",
    "    logistic_models = []\n",
    "    for i in range(9):\n",
    "        X_train_resampled, y_train_resampled = resample(X_train, y_train, replace=True, random_state=i)\n",
    "        # print(X_train_resampled.shape, y_train_resampled.shape)\n",
    "        weights, bias = LogisticRegression.fit(X_train_resampled, y_train_resampled, learning_rate=0.5, n_iters=1000)\n",
    "        logistic_models.append((weights, bias))\n",
    "        # print(\"Model\", i, \"trained weights and bias\", weights)\n",
    "        # print(\"Model\", i, \"trained bias\", bias)\n",
    "    return logistic_models\n",
    "\n",
    "def create_prediction_matrix(logistic_models, X_val):\n",
    "    n_models = len(logistic_models)\n",
    "    n_samples = X_val.shape[0]\n",
    "    \n",
    "    # Initialize an empty matrix to hold predictions\n",
    "    prediction_matrix = np.zeros((n_models, n_samples))\n",
    "    \n",
    "    # Fill the matrix with predictions from each model\n",
    "    for i, (weights, bias) in enumerate(logistic_models):\n",
    "        # Get predictions from the ith model and assign it to the ith row\n",
    "        predictions = LogisticRegression.predict(X_val, weights, bias)\n",
    "        prediction_matrix[i, :] = predictions\n",
    "    \n",
    "    return prediction_matrix\n",
    "\n",
    "logistic_models = bagging(X_train, y_train)\n",
    "# prediction_matrix_df = pd.DataFrame(prediction_matrix)\n",
    "# print(prediction_matrix_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> testing all the 9 models on test set </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm, roc_auc_score, precision_recall_curve, auc\n",
    "accuracy = []\n",
    "senstivity = []\n",
    "specificity = []\n",
    "precision = []\n",
    "f1_score = []\n",
    "auroc = []\n",
    "aupr = []\n",
    "for weights,bias in logistic_models:\n",
    "    predictions = LogisticRegression.predict(X_test, weights, bias)\n",
    "    accuracy.append(np.mean(predictions == y_test))\n",
    "    tn, fp, fn, tp = cm(y_test, predictions).ravel()\n",
    "    senstivity.append(tp/(tp+fn))\n",
    "    specificity.append(tn/(tn+fp))\n",
    "    precision.append(tp/(tp+fp))\n",
    "    # print(precision, senstivity)\n",
    "    f1_score.append(2*tp/(2*tp+fp+fn))\n",
    "    # auroc.append(roc_auc_score(y_test, predictions))\n",
    "    precision_, recall_, x = precision_recall_curve(y_test, predictions)\n",
    "    # print(precision_, recall_)\n",
    "    aupr.append(auc(recall_, precision_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Violin Plots </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAccuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maccuracy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPrecision\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSensitivity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msenstivity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSpecificity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mspecificity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF1 Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf1_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAUROC\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mauroc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAUPR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maupr\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "metrics = pd.DataFrame({\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Sensitivity': senstivity,\n",
    "    'Specificity': specificity,\n",
    "    'F1 Score': f1_score,\n",
    "    'AUROC': auroc,\n",
    "    'AUPR': aupr\n",
    "})\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "metrics_melted = metrics.melt(var_name='Metric', value_name='Score')\n",
    "\n",
    "# Define a color palette for the metrics\n",
    "color_palette = {\n",
    "    'Accuracy': 'skyblue',\n",
    "    'Precision': 'lightgreen',\n",
    "    'Sensitivity': 'salmon',\n",
    "    'Specificity': 'red',\n",
    "    'F1 Score': 'gold',\n",
    "    'AUROC': 'violet',\n",
    "    'AUPR': 'deepskyblue'\n",
    "}\n",
    "\n",
    "# Draw the violin plot with the specified colors\n",
    "sns.violinplot(x='Metric', y='Score', data=metrics_melted, palette=color_palette)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Performance Metrics for Bagging LR Learners', fontsize=16)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xlabel('Metrics', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Taking the average and std deviation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3904: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "c:\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "c:\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:227: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "c:\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean,\n",
      "c:\\Python312\\Lib\\site-packages\\numpy\\_core\\_methods.py:219: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# print(f\"Accuracy: {np.mean(accuracy)} ± {np.std(accuracy)}\")\n",
    "# print(f\"Senstivity: {np.mean(senstivity)} ± {np.std(senstivity)}\")\n",
    "# print(f\"Specificity: {np.mean(specificity)} ± {np.std(specificity)}\")\n",
    "# print(f\"Precision: {np.mean(precision)} ± {np.std(precision)}\")\n",
    "# print(f\"F1 Score: {np.mean(f1_score)} ± {np.std(f1_score)}\")\n",
    "# print(f\"AUROC: {np.mean(auroc)} ± {np.std(auroc)}\")\n",
    "# print(f\"AUPR: {np.mean(aupr)} ± {np.std(aupr)}\")\n",
    "\n",
    "accuracy = str(np.mean(accuracy)) + ' ± ' + str(np.std(accuracy))\n",
    "senstivity = str(np.mean(senstivity)) + ' ± ' + str(np.std(senstivity))\n",
    "specificity = str(np.mean(specificity)) + ' ± ' + str(np.std(specificity))\n",
    "precision = str(np.mean(precision)) + ' ± ' + str(np.std(precision))\n",
    "f1_score = str(np.mean(f1_score)) + ' ± ' + str(np.std(f1_score))\n",
    "auroc = str(np.mean(auroc)) + ' ± ' + str(np.std(auroc))\n",
    "aupr = str(np.mean(aupr)) + ' ± ' + str(np.std(aupr))\n",
    "\n",
    "lrdata = {\n",
    "    '': ['LR'],\n",
    "    'Accuracy': [accuracy],\n",
    "    'Sensitivity': [senstivity],\n",
    "    'Specificity': [specificity],\n",
    "     'Precision': [precision],\n",
    "    'F1 Score': [f1_score],\n",
    "    'AUROC': [auroc],\n",
    "    'AUPR': [aupr]\n",
    "}\n",
    "\n",
    "lrdata = pd.DataFrame(lrdata)\n",
    "# print(lrdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def majority_voting(logistic_models, X_test):\n",
    "    prediction_matrix = create_prediction_matrix(logistic_models, X_test)\n",
    "    prediction_matrix = prediction_matrix.T\n",
    "    majority_voting_predictions = stats.mode(prediction_matrix, axis=1)[0].flatten()\n",
    "    pm = pd.DataFrame(prediction_matrix)\n",
    "    mv = pd.DataFrame(majority_voting_predictions)\n",
    "    dt = pd.concat([pm, mv], axis=1)\n",
    "    # print(dt.head(20))\n",
    "    return majority_voting_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Majority Voting </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "y_pred = majority_voting(logistic_models, X_test)\n",
    "# ydf = pd.DataFrame(y_pred)\n",
    "# ydf.columns = ['Class']\n",
    "# yt = pd.DataFrame(y_test)\n",
    "# yt.columns = ['Class_test']\n",
    "# dt = pd.concat([ydf, yt], axis=1)\n",
    "# print(dt)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "tn, fp, fn, tp = cm(y_test, y_pred).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn+fp)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "precision_, recall_, x_ = precision_recall_curve(y_test, y_pred)\n",
    "aupr_voting = auc(recall_, precision_)\n",
    "\n",
    "# print(\"Output of Voting Ensemble :\")\n",
    "data = {\n",
    "    '': ['Voting ensemble'],\n",
    "    'Accuracy' : [accuracy], \n",
    "    'Sensitivity' : [recall],\n",
    "    'Specificity' : [specificity],\n",
    "    'Precision' : [precision],\n",
    "    'F1 Score' : [f1_score],\n",
    "    'AUROC' : [roc_auc_score], \n",
    "    'AUPR' : [aupr_voting]\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame\n",
    "data = pd.DataFrame(data)\n",
    "# metrics_df = metrics_df.style.set_table_styles(\n",
    "#     [{'selector': 'table', 'props': [('border', '1px solid black')]},\n",
    "#      {'selector': 'th', 'props': [('border', '1px solid black')]},\n",
    "#      {'selector': 'td', 'props': [('border', '1px solid black')]}]\n",
    "# )\n",
    "# # Display the table\n",
    "# metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Stacking </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_meta_classifier(y_val, prediction_matrix):\n",
    "    prediction_matrix = np.concatenate((X_val,prediction_matrix.T),axis=1)\n",
    "    prediction_matrix_df = pd.DataFrame(prediction_matrix)\n",
    "    # print(prediction_matrix_df)\n",
    "    weights, bias = LogisticRegression.fit(prediction_matrix, y_val, learning_rate=0.5, n_iters=1000)\n",
    "    return weights, bias\n",
    "\n",
    "prediction_matrix = create_prediction_matrix(logistic_models, X_val)\n",
    "weights, bias = train_meta_classifier(y_val, prediction_matrix)\n",
    "# print(\"Meta classifier trained weights and bias\", weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_meta_classifier(logistic_models, weights, bias, X_test):\n",
    "    prediction_matrix = create_prediction_matrix(logistic_models, X_test)\n",
    "    prediction_matrix = np.concatenate((X_test,prediction_matrix.T),axis=1) \n",
    "    y_pred = LogisticRegression.predict(prediction_matrix, weights, bias)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b9598 table {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_b9598 th {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_b9598 td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b9598\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b9598_level0_col0\" class=\"col_heading level0 col0\" ></th>\n",
       "      <th id=\"T_b9598_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
       "      <th id=\"T_b9598_level0_col2\" class=\"col_heading level0 col2\" >Sensitivity</th>\n",
       "      <th id=\"T_b9598_level0_col3\" class=\"col_heading level0 col3\" >Specificity</th>\n",
       "      <th id=\"T_b9598_level0_col4\" class=\"col_heading level0 col4\" >Precision</th>\n",
       "      <th id=\"T_b9598_level0_col5\" class=\"col_heading level0 col5\" >F1 Score</th>\n",
       "      <th id=\"T_b9598_level0_col6\" class=\"col_heading level0 col6\" >AUROC</th>\n",
       "      <th id=\"T_b9598_level0_col7\" class=\"col_heading level0 col7\" >AUPR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b9598_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b9598_row0_col0\" class=\"data row0 col0\" >LR</td>\n",
       "      <td id=\"T_b9598_row0_col1\" class=\"data row0 col1\" >0.8338053479828482 ± 0.0009628215072652968</td>\n",
       "      <td id=\"T_b9598_row0_col2\" class=\"data row0 col2\" >0.5111582920500118 ± 0.0071637384870898045</td>\n",
       "      <td id=\"T_b9598_row0_col3\" class=\"data row0 col3\" >0.9362917003866558 ± 0.0018406159059849558</td>\n",
       "      <td id=\"T_b9598_row0_col4\" class=\"data row0 col4\" >0.7182449000656574 ± 0.0037950714417612176</td>\n",
       "      <td id=\"T_b9598_row0_col5\" class=\"data row0 col5\" >0.5972164896409535 ± 0.004345605749975847</td>\n",
       "      <td id=\"T_b9598_row0_col6\" class=\"data row0 col6\" >nan ± nan</td>\n",
       "      <td id=\"T_b9598_row0_col7\" class=\"data row0 col7\" >0.673623810318364 ± 0.002367444058341465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9598_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b9598_row1_col0\" class=\"data row1 col0\" >Voting ensemble</td>\n",
       "      <td id=\"T_b9598_row1_col1\" class=\"data row1 col1\" >0.833965</td>\n",
       "      <td id=\"T_b9598_row1_col2\" class=\"data row1 col2\" >0.508705</td>\n",
       "      <td id=\"T_b9598_row1_col3\" class=\"data row1 col3\" >0.937281</td>\n",
       "      <td id=\"T_b9598_row1_col4\" class=\"data row1 col4\" >0.720385</td>\n",
       "      <td id=\"T_b9598_row1_col5\" class=\"data row1 col5\" >0.596317</td>\n",
       "      <td id=\"T_b9598_row1_col6\" class=\"data row1 col6\" >0.722993</td>\n",
       "      <td id=\"T_b9598_row1_col7\" class=\"data row1 col7\" >0.673763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b9598_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b9598_row2_col0\" class=\"data row2 col0\" >Stacking ensemble</td>\n",
       "      <td id=\"T_b9598_row2_col1\" class=\"data row2 col1\" >0.834067</td>\n",
       "      <td id=\"T_b9598_row2_col2\" class=\"data row2 col2\" >0.515499</td>\n",
       "      <td id=\"T_b9598_row2_col3\" class=\"data row2 col3\" >0.935258</td>\n",
       "      <td id=\"T_b9598_row2_col4\" class=\"data row2 col4\" >0.716647</td>\n",
       "      <td id=\"T_b9598_row2_col5\" class=\"data row2 col5\" >0.599654</td>\n",
       "      <td id=\"T_b9598_row2_col6\" class=\"data row2 col6\" >0.725378</td>\n",
       "      <td id=\"T_b9598_row2_col7\" class=\"data row2 col7\" >0.674472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x258af16eff0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm , f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "y_pred = test_meta_classifier(logistic_models, weights, bias, X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "tn, fp, fn, tp = cm(y_test, y_pred).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn+fp)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "precision_, recall_, x_ = precision_recall_curve(y_test, y_pred)\n",
    "aupr_stack = auc(recall_, precision_)\n",
    "stdata = {\n",
    "    '': ['Stacking ensemble'],\n",
    "    'Accuracy' : [accuracy], \n",
    "    'Sensitivity' : [recall],\n",
    "    'Specificity' : [specificity],\n",
    "    'Precision' : [precision],\n",
    "    'F1 Score' : [f1_score],\n",
    "    'AUROC' : [roc_auc_score], \n",
    "    'AUPR' : [aupr_stack]\n",
    "}\n",
    "\n",
    "stdata = pd.DataFrame(stdata)\n",
    "\n",
    "combined_df = pd.concat([ lrdata, data, stdata], ignore_index=True)\n",
    "# Convert to a DataFrame\n",
    "metrics_df = pd.DataFrame(combined_df)\n",
    "metrics_df = metrics_df.style.set_table_styles(\n",
    "    [{'selector': 'table', 'props': [('border', '1px solid black')]},\n",
    "     {'selector': 'th', 'props': [('border', '1px solid black')]},\n",
    "     {'selector': 'td', 'props': [('border', '1px solid black')]}]\n",
    ")\n",
    "# Display the table\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
