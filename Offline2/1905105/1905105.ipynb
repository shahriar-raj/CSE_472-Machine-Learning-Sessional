{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1: Load the Telco data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the churn dataset\n",
    "def Load_telco():\n",
    "    data = pd.read_csv('WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    # Preview the data\n",
    "    # data.head(10)\n",
    "    data.drop('customerID', axis=1, inplace=True) # Drop the customerID column\n",
    "    Features = data.drop('Churn', axis=1) # Features\n",
    "    Labels = data['Churn'] # Labels\n",
    "    map_Labels = {'Yes': 1, 'No': 0} # Map the labels to 0 and 1\n",
    "    Labels = [map_Labels[i] for i in Labels] # Replace the labels with 0 and 1\n",
    "    Labels = pd.Series(Labels) # Convert the labels to a pandas series\n",
    "    Features['TotalCharges'] = pd.to_numeric(Features['TotalCharges'], errors='coerce') # Convert TotalCharges to numeric\n",
    "    Features['MultipleLines'] = Features['MultipleLines'].replace('No phone service', 'No') # Replace 'No phone service' with 'No'\n",
    "    # Features.isnull().sum() # Count the number of missing values in each column\n",
    "    Features.fillna(Features.mean(numeric_only=True), inplace=True) # Fill the missing values with the mean of the column\n",
    "    # Features\n",
    "    categorical_columns = Features.select_dtypes(include=['object']).columns # Select the categorical columns\n",
    "    for column in categorical_columns:\n",
    "        Features[column] = Features[column].astype('category')\n",
    "    Features = pd.get_dummies(Features) # One-hot encode the categorical columns\n",
    "    scaler = MinMaxScaler() # Create a MinMaxScaler object\n",
    "    Features = pd.DataFrame(scaler.fit_transform(Features), columns=Features.columns) # Normalize the features\n",
    "    Labels_array = Labels.to_numpy() # Convert the labels to a numpy array\n",
    "    return Features, Labels_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1: Load the Adult data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the adult.data file\n",
    "def Load_adult():\n",
    "    train = pd.read_csv('adult/adult.data', header=None, skipinitialspace=True)\n",
    "    test = pd.read_csv('adult/adult.test', header=None, skipinitialspace=True, skiprows=1)\n",
    "    data = pd.concat([train, test])\n",
    "    data.columns =  ['age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "                        'marital-status', 'occupation', 'relationship', 'race',\n",
    "                        'sex', 'capital-gain', 'capital-loss', 'hours-per-week',\n",
    "                        'native-country', 'income']\n",
    "    # for column in data.columns:\n",
    "    #     if data[column].dtype == 'object':\n",
    "    #         print (column)\n",
    "    #         print(data[column].unique()) # Print the unique values in the column.\n",
    "    #         There are missing values in the dataset. The missing values are represented by a question mark '?' in workclass, occupation and native-country columns.\n",
    "    data = data.replace('?', np.nan) # Replace '?' with np.nan\n",
    "    data = data.apply(lambda x: x.str.rstrip('.') if x.dtype == \"object\" else x) # Remove the '.' at the end of the values in the columns\n",
    "    # data\n",
    "    Features = data.drop('income', axis=1) # Features\n",
    "    Labels = data['income'] # Labels\n",
    "    map_Labels = {'>50K': 1, '<=50K': 0} # Map the labels to 0 and 1\n",
    "    Labels = [map_Labels[i] for i in Labels] # Replace the labels with 0 and 1\n",
    "    # Features.isnull().sum() # Count the number of missing values in each column\n",
    "    Features.fillna(Features.mode().iloc[0], inplace=True) # Fill the missing values with the mode of the column\n",
    "    # Features.isnull().sum() # Count the number of missing values in each column\n",
    "    categorical_columns = Features.select_dtypes(include=['object']).columns # Select the categorical columns\n",
    "    for column in categorical_columns:\n",
    "        Features[column] = Features[column].astype('category')\n",
    "    Features = pd.get_dummies(Features) # One-hot encode the categorical columns\n",
    "    scaler = MinMaxScaler() # Create a MinMaxScaler object\n",
    "    Features = pd.DataFrame(scaler.fit_transform(Features), columns=Features.columns) # Normalize the features  \n",
    "    # Features\n",
    "    Labels_array = np.array(Labels) # Convert the labels to a numpy array\n",
    "    # Labels_array\n",
    "    return Features, Labels_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Step 1: Load the CreditCard data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the creditcard.csv file\n",
    "def Load_creditcard():\n",
    "    data = pd.read_csv('creditcard.csv')\n",
    "    # data['Class'].value_counts()\n",
    "    # data\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    frauds = data[data['Class'] == 1]\n",
    "    not_frauds = data[data['Class'] == 0]\n",
    "    not_frauds = not_frauds.sample(n=20000, random_state=42)\n",
    "    data = pd.concat([frauds, not_frauds])\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    Features = data.drop('Class', axis=1) # Features\n",
    "    Labels = data['Class'] # Labels\n",
    "    scaler = MinMaxScaler() # Create a MinMaxScaler object\n",
    "    Features = pd.DataFrame(scaler.fit_transform(Features), columns=Features.columns) # Normalize the features\n",
    "    Labels_array = np.array(Labels) # Convert the labels to a numpy array\n",
    "    return Features, Labels_array\n",
    "    # Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Logistic Regression Class</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    @staticmethod\n",
    "    def initialize_params(n_features):\n",
    "        weights = np.zeros(n_features)\n",
    "        bias = 0\n",
    "        return weights, bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_loss(y, y_pred):\n",
    "        # Binary cross-entropy loss\n",
    "        n_samples = y.shape[0]\n",
    "        loss = -(1 / n_samples) * np.sum(y * np.log(y_pred) + (1 - y) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient_descent(X, y, weights, bias, learning_rate, n_iters):\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        for i in range(n_iters):\n",
    "            # Linear model: z = X * weights + bias\n",
    "            z = np.dot(X, weights) + bias\n",
    "            y_pred = LogisticRegression.sigmoid(z)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "            # print(\"iteration:\", i) # Print the iteration    \n",
    "            # print(LogisticRegression.compute_loss(y, y_pred)) # Print the loss\n",
    "            # Update weights and bias\n",
    "            weights -= learning_rate * dw\n",
    "            bias -= learning_rate * db\n",
    "            \n",
    "        return weights, bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit(X, y, learning_rate=0.01, n_iters=1000):\n",
    "        n_features = X.shape[1]\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        weights, bias = LogisticRegression.initialize_params(n_features)\n",
    "        \n",
    "        # Perform gradient descent\n",
    "        weights, bias = LogisticRegression.gradient_descent(X, y, weights, bias, learning_rate, n_iters)\n",
    "        \n",
    "        return weights, bias\n",
    "    \n",
    "    @staticmethod\n",
    "    def predict(X, weights, bias):\n",
    "        # Linear model and applying sigmoid to get probabilities\n",
    "        z = np.dot(X, weights) + bias\n",
    "        y_pred = LogisticRegression.sigmoid(z)\n",
    "        \n",
    "        # Convert probabilities to binary class (0 or 1)\n",
    "        return [1 if i > 0.5 else 0 for i in y_pred]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Splitting the dataset into Train, Validation & Test Dataset </b>\n",
    "Here for 3 datasets, we have to comment the 2 datasets not needed and uncomment the target dataset in 2,3 & 4 number lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Features, Labels_array = Load_telco()\n",
    "# Features, Labels_array = Load_adult()\n",
    "# Features, Labels_array = Load_creditcard()\n",
    "# Split the data into training+validation and testing sets\n",
    "X_tr_val, X_test, y_tr_val, y_test = train_test_split(Features, Labels_array, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split the training+validation set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tr_val, y_tr_val, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Training the model\n",
    "# weights, bias = LogisticRegression.fit(X_train, y_train, learning_rate=0.95, n_iters=1000) # lr = 0.95 (creditcard), lr = 0.8 (adult), lr = 0.5 (telco)\n",
    "\n",
    "# # Predicting\n",
    "# predictions = LogisticRegression.predict(X_test, weights, bias)\n",
    "\n",
    "# print(\"Predictions:\", predictions)\n",
    "# # Evaluate the model\n",
    "# accuracy = np.mean(predictions == y_test)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Bagging </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def bagging(X_train, y_train):\n",
    "    logistic_models = []\n",
    "    for i in range(9):\n",
    "        X_train_resampled, y_train_resampled = resample(X_train, y_train, replace=True, random_state=i)\n",
    "        # print(X_train_resampled.shape, y_train_resampled.shape)\n",
    "        weights, bias = LogisticRegression.fit(X_train_resampled, y_train_resampled, learning_rate=0.5, n_iters=1000)\n",
    "        logistic_models.append((weights, bias))\n",
    "        # print(\"Model\", i, \"trained weights and bias\", weights)\n",
    "        # print(\"Model\", i, \"trained bias\", bias)\n",
    "    return logistic_models\n",
    "\n",
    "def create_prediction_matrix(logistic_models, X_val):\n",
    "    n_models = len(logistic_models)\n",
    "    n_samples = X_val.shape[0]\n",
    "    \n",
    "    # Initialize an empty matrix to hold predictions\n",
    "    prediction_matrix = np.zeros((n_models, n_samples))\n",
    "    \n",
    "    # Fill the matrix with predictions from each model\n",
    "    for i, (weights, bias) in enumerate(logistic_models):\n",
    "        # Get predictions from the ith model and assign it to the ith row\n",
    "        predictions = LogisticRegression.predict(X_val, weights, bias)\n",
    "        prediction_matrix[i, :] = predictions\n",
    "    \n",
    "    return prediction_matrix\n",
    "\n",
    "logistic_models = bagging(X_train, y_train)\n",
    "# prediction_matrix_df = pd.DataFrame(prediction_matrix)\n",
    "# print(prediction_matrix_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> testing all the 9 models on test set </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm, roc_auc_score, precision_recall_curve, auc\n",
    "accuracy = []\n",
    "senstivity = []\n",
    "specificity = []\n",
    "precision = []\n",
    "f1_score = []\n",
    "auroc = []\n",
    "aupr = []\n",
    "for weights,bias in logistic_models:\n",
    "    predictions = LogisticRegression.predict(X_test, weights, bias)\n",
    "    accuracy.append(np.mean(predictions == y_test))\n",
    "    tn, fp, fn, tp = cm(y_test, predictions).ravel()\n",
    "    senstivity.append(tp/(tp+fn))\n",
    "    specificity.append(tn/(tn+fp))\n",
    "    precision.append(tp/(tp+fp))\n",
    "    # print(precision, senstivity)\n",
    "    f1_score.append(2*tp/(2*tp+fp+fn))\n",
    "    auroc.append(roc_auc_score(y_test, predictions))\n",
    "    precision_, recall_, x = precision_recall_curve(y_test, predictions)\n",
    "    # print(precision_, recall_)\n",
    "    aupr.append(auc(recall_, precision_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Violin Plots </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({\n",
    "    'Accuracy': accuracy,\n",
    "    'Precision': precision,\n",
    "    'Sensitivity': senstivity,\n",
    "    'Specificity': specificity,\n",
    "    'F1 Score': f1_score,\n",
    "    'AUROC': auroc,\n",
    "    'AUPR': aupr\n",
    "})\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Melt the DataFrame to long format\n",
    "metrics_melted = metrics.melt(var_name='Metric', value_name='Score')\n",
    "\n",
    "# Define a color palette for the metrics\n",
    "color_palette = {\n",
    "    'Accuracy': 'skyblue',\n",
    "    'Precision': 'lightgreen',\n",
    "    'Sensitivity': 'salmon',\n",
    "    'Specificity': 'red',\n",
    "    'F1 Score': 'gold',\n",
    "    'AUROC': 'violet',\n",
    "    'AUPR': 'deepskyblue'\n",
    "}\n",
    "\n",
    "# Draw the violin plot with the specified colors\n",
    "sns.violinplot(x='Metric', y='Score', data=metrics_melted, palette=color_palette)\n",
    "\n",
    "# Set plot title and labels\n",
    "plt.title('Performance Metrics for Bagging LR Learners', fontsize=16)\n",
    "plt.ylabel('Score', fontsize=14)\n",
    "plt.xlabel('Metrics', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Taking the average and std deviation </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Accuracy: {np.mean(accuracy)} ± {np.std(accuracy)}\")\n",
    "# print(f\"Senstivity: {np.mean(senstivity)} ± {np.std(senstivity)}\")\n",
    "# print(f\"Specificity: {np.mean(specificity)} ± {np.std(specificity)}\")\n",
    "# print(f\"Precision: {np.mean(precision)} ± {np.std(precision)}\")\n",
    "# print(f\"F1 Score: {np.mean(f1_score)} ± {np.std(f1_score)}\")\n",
    "# print(f\"AUROC: {np.mean(auroc)} ± {np.std(auroc)}\")\n",
    "# print(f\"AUPR: {np.mean(aupr)} ± {np.std(aupr)}\")\n",
    "\n",
    "accuracy = str(np.mean(accuracy)) + ' ± ' + str(np.std(accuracy))\n",
    "senstivity = str(np.mean(senstivity)) + ' ± ' + str(np.std(senstivity))\n",
    "specificity = str(np.mean(specificity)) + ' ± ' + str(np.std(specificity))\n",
    "precision = str(np.mean(precision)) + ' ± ' + str(np.std(precision))\n",
    "f1_score = str(np.mean(f1_score)) + ' ± ' + str(np.std(f1_score))\n",
    "auroc = str(np.mean(auroc)) + ' ± ' + str(np.std(auroc))\n",
    "aupr = str(np.mean(aupr)) + ' ± ' + str(np.std(aupr))\n",
    "\n",
    "lrdata = {\n",
    "    '': ['LR'],\n",
    "    'Accuracy': [accuracy],\n",
    "    'Sensitivity': [senstivity],\n",
    "    'Specificity': [specificity],\n",
    "     'Precision': [precision],\n",
    "    'F1 Score': [f1_score],\n",
    "    'AUROC': [auroc],\n",
    "    'AUPR': [aupr]\n",
    "}\n",
    "\n",
    "lrdata = pd.DataFrame(lrdata)\n",
    "# print(lrdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def majority_voting(logistic_models, X_test):\n",
    "    prediction_matrix = create_prediction_matrix(logistic_models, X_test)\n",
    "    prediction_matrix = prediction_matrix.T\n",
    "    majority_voting_predictions = stats.mode(prediction_matrix, axis=1)[0].flatten()\n",
    "    pm = pd.DataFrame(prediction_matrix)\n",
    "    mv = pd.DataFrame(majority_voting_predictions)\n",
    "    dt = pd.concat([pm, mv], axis=1)\n",
    "    # print(dt.head(20))\n",
    "    return majority_voting_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Majority Voting </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm, f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "y_pred = majority_voting(logistic_models, X_test)\n",
    "# ydf = pd.DataFrame(y_pred)\n",
    "# ydf.columns = ['Class']\n",
    "# yt = pd.DataFrame(y_test)\n",
    "# yt.columns = ['Class_test']\n",
    "# dt = pd.concat([ydf, yt], axis=1)\n",
    "# print(dt)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "tn, fp, fn, tp = cm(y_test, y_pred).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn+fp)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "precision_, recall_, x_ = precision_recall_curve(y_test, y_pred)\n",
    "aupr_voting = auc(recall_, precision_)\n",
    "\n",
    "# print(\"Output of Voting Ensemble :\")\n",
    "data = {\n",
    "    '': ['Voting ensemble'],\n",
    "    'Accuracy' : [accuracy], \n",
    "    'Sensitivity' : [recall],\n",
    "    'Specificity' : [specificity],\n",
    "    'Precision' : [precision],\n",
    "    'F1 Score' : [f1_score],\n",
    "    'AUROC' : [roc_auc_score], \n",
    "    'AUPR' : [aupr_voting]\n",
    "}\n",
    "\n",
    "# Convert to a DataFrame\n",
    "data = pd.DataFrame(data)\n",
    "# metrics_df = metrics_df.style.set_table_styles(\n",
    "#     [{'selector': 'table', 'props': [('border', '1px solid black')]},\n",
    "#      {'selector': 'th', 'props': [('border', '1px solid black')]},\n",
    "#      {'selector': 'td', 'props': [('border', '1px solid black')]}]\n",
    "# )\n",
    "# # Display the table\n",
    "# metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b> Stacking </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_meta_classifier(y_val, prediction_matrix):\n",
    "    prediction_matrix = np.concatenate((X_val,prediction_matrix.T),axis=1)\n",
    "    prediction_matrix_df = pd.DataFrame(prediction_matrix)\n",
    "    # print(prediction_matrix_df)\n",
    "    weights, bias = LogisticRegression.fit(prediction_matrix, y_val, learning_rate=0.5, n_iters=1000)\n",
    "    return weights, bias\n",
    "\n",
    "prediction_matrix = create_prediction_matrix(logistic_models, X_val)\n",
    "weights, bias = train_meta_classifier(y_val, prediction_matrix)\n",
    "# print(\"Meta classifier trained weights and bias\", weights, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_meta_classifier(logistic_models, weights, bias, X_test):\n",
    "    prediction_matrix = create_prediction_matrix(logistic_models, X_test)\n",
    "    prediction_matrix = np.concatenate((X_test,prediction_matrix.T),axis=1) \n",
    "    y_pred = LogisticRegression.predict(prediction_matrix, weights, bias)\n",
    "    return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix as cm , f1_score, roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "y_pred = test_meta_classifier(logistic_models, weights, bias, X_test)\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "tn, fp, fn, tp = cm(y_test, y_pred).ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "specificity = tn / (tn+fp)\n",
    "f1_score = f1_score(y_test, y_pred)\n",
    "roc_auc_score = roc_auc_score(y_test, y_pred)\n",
    "precision_, recall_, x_ = precision_recall_curve(y_test, y_pred)\n",
    "aupr_stack = auc(recall_, precision_)\n",
    "stdata = {\n",
    "    '': ['Stacking ensemble'],\n",
    "    'Accuracy' : [accuracy], \n",
    "    'Sensitivity' : [recall],\n",
    "    'Specificity' : [specificity],\n",
    "    'Precision' : [precision],\n",
    "    'F1 Score' : [f1_score],\n",
    "    'AUROC' : [roc_auc_score], \n",
    "    'AUPR' : [aupr_stack]\n",
    "}\n",
    "\n",
    "stdata = pd.DataFrame(stdata)\n",
    "\n",
    "combined_df = pd.concat([ lrdata, data, stdata], ignore_index=True)\n",
    "# Convert to a DataFrame\n",
    "metrics_df = pd.DataFrame(combined_df)\n",
    "metrics_df = metrics_df.style.set_table_styles(\n",
    "    [{'selector': 'table', 'props': [('border', '1px solid black')]},\n",
    "     {'selector': 'th', 'props': [('border', '1px solid black')]},\n",
    "     {'selector': 'td', 'props': [('border', '1px solid black')]}]\n",
    ")\n",
    "# Display the table\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
