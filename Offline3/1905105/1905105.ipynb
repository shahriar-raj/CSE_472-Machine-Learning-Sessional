{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (48000, 784)\n",
      "Validation data shape: (12000, 784)\n",
      "Test data shape: (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# Data transformation to convert images to tensors and normalize\n",
    "transform = transforms.ToTensor()\n",
    "np.random.seed(42)\n",
    "# Load the FashionMNIST dataset\n",
    "train_data = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_data = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the datasets into numpy arrays\n",
    "def dataset_to_numpy(dataset):\n",
    "    X = []\n",
    "    y = []\n",
    "    for img, label in dataset:\n",
    "        X.append(img.numpy().reshape(-1))  # Flatten the image\n",
    "        y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Prepare numpy arrays for training and validation sets\n",
    "X_train, y_train = dataset_to_numpy(train_data)\n",
    "X_val, y_val = dataset_to_numpy(val_data)\n",
    "X_test, y_test = dataset_to_numpy(test_data)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Validation data shape:\", X_val.shape)\n",
    "print(\"Test data shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Layer With Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam:\n",
    "    def __init__(self, shape, learning_rate=0.005, epsilon=1e-8, beta1=0.9, beta2=0.999):\n",
    "        self.v = np.zeros(shape)\n",
    "        self.s = np.zeros(shape)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0\n",
    "\n",
    "    def update(self, w, grad_w):\n",
    "        self.t += 1\n",
    "        self.v = self.beta1 * self.v + (1 - self.beta1) * grad_w\n",
    "        self.s = self.beta2 * self.s + (1 - self.beta2) * (grad_w ** 2)\n",
    "        v_norm = self.v / (1 - self.beta1 ** self.t)\n",
    "        s_norm = self.s / (1 - self.beta2 ** self.t)\n",
    "        w -= self.learning_rate * v_norm / (np.sqrt(s_norm) + self.epsilon)\n",
    "        return w\n",
    "\n",
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2. / input_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        self.weight_optimizer = Adam(self.weights.shape)\n",
    "        self.bias_optimizer = Adam(self.biases.shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        self.output = np.dot(X, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        grad_weights = np.dot(self.input.T, d_out)\n",
    "        grad_biases = np.sum(d_out, axis=0, keepdims=True)\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "\n",
    "        # Update weights and biases using Adam optimizer\n",
    "        self.weights = self.weight_optimizer.update(self.weights, grad_weights)\n",
    "        self.biases = self.bias_optimizer.update(self.biases, grad_biases)\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization (batch Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, input_size, epsilon=1e-5):\n",
    "        momentum=0.9\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.gamma = np.ones((1, input_size))\n",
    "        self.beta = np.zeros((1, input_size))\n",
    "        self.moving_mean = np.zeros((1, input_size))\n",
    "        self.moving_var = np.zeros((1, input_size))\n",
    "        self.gamma_optimizer = Adam(self.gamma.shape)\n",
    "        self.beta_optimizer = Adam(self.beta.shape)\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        self.input = X\n",
    "        if training:\n",
    "            self.mean = np.mean(X, axis=0, keepdims=True)\n",
    "            self.var = np.var(X, axis=0, keepdims=True)\n",
    "            self.std = np.sqrt(self.var + self.epsilon)\n",
    "            self.x_norm = (X - self.mean) / self.std\n",
    "            self.moving_mean = self.momentum * self.moving_mean + (1 - self.momentum) * self.mean\n",
    "            self.moving_var = self.momentum * self.moving_var + (1 - self.momentum) * self.var\n",
    "        else:\n",
    "            self.x_norm = (X - self.moving_mean) / np.sqrt(self.moving_var + self.epsilon)\n",
    "        output = self.gamma * self.x_norm + self.beta\n",
    "        return output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        m = self.input.shape[0]\n",
    "        self.d_gamma = np.sum(d_out * self.x_norm, axis=0, keepdims=True)\n",
    "        self.d_beta = np.sum(d_out, axis=0, keepdims=True)\n",
    "\n",
    "        dx_norm = d_out * self.gamma\n",
    "        d_var = np.sum(dx_norm * (self.input - self.mean) * -0.5 * (self.var + self.epsilon) ** (-1.5), axis=0)\n",
    "        d_mean = np.sum(dx_norm * -1 / self.std, axis=0) + d_var * np.mean(-2 * (self.input - self.mean), axis=0)\n",
    "        d_input = dx_norm / self.std + d_var * 2 * (self.input - self.mean) / m + d_mean / m\n",
    "\n",
    "        # Update gamma and beta using Adam optimizer\n",
    "        self.gamma = self.gamma_optimizer.update(self.gamma, self.d_gamma)\n",
    "        self.beta = self.beta_optimizer.update(self.beta, self.d_beta)\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation (ReLU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        return np.maximum(0, X)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_input = d_out.copy()\n",
    "        d_input[self.input <= 0] = 0\n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization (Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.rate = dropout_rate\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            self.mask = (np.random.rand(*X.shape) > self.rate) / (1 - self.rate)\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        return d_out * self.mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression (SoftMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxCrossEntropyLoss:\n",
    "    def forward(self, logits, targets):\n",
    "        # Compute softmax probabilities\n",
    "        exp_values = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.probabilities = probabilities\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        samples = logits.shape[0]\n",
    "        correct_logprobs = -np.log(self.probabilities[range(samples), targets])\n",
    "        loss = np.sum(correct_logprobs) / samples\n",
    "        return loss\n",
    "\n",
    "    def backward(self, targets):\n",
    "        samples = self.probabilities.shape[0]\n",
    "        d_logits = self.probabilities.copy()\n",
    "        d_logits[range(samples), targets] -= 1\n",
    "        d_logits /= samples\n",
    "        return d_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, (Dropout, BatchNormalization)):\n",
    "                X = layer.forward(X, training)\n",
    "            else:\n",
    "                X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        for layer in reversed(self.layers):\n",
    "            d_out = layer.backward(d_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_train.shape[1]  # Number of features\n",
    "num_classes = len(np.unique(y_train))  # Number of classes\n",
    "\n",
    "# Example architecture\n",
    "layers = [\n",
    "    DenseLayer(input_size, 128),\n",
    "    BatchNormalization(128),\n",
    "    ReLU(),\n",
    "    Dropout(0.5),\n",
    "    # DenseLayer(128, 64),\n",
    "    # BatchNormalization(64),\n",
    "    # ReLU(),\n",
    "    # Dropout(0.5),\n",
    "    DenseLayer(128, num_classes)\n",
    "]\n",
    "\n",
    "model = NeuralNetwork(layers)\n",
    "loss_function = SoftmaxCrossEntropyLoss()\n",
    "# print(input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 750/750 [00:18<00:00, 41.09it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "Train Loss: 0.5747, Train Accuracy: 0.7941\n",
      "Val Loss: 0.4091, Val Accuracy: 0.8505, Val Macro-F1: 0.8441\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 750/750 [00:06<00:00, 112.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "Train Loss: 0.4614, Train Accuracy: 0.8333\n",
      "Val Loss: 0.3848, Val Accuracy: 0.8592, Val Macro-F1: 0.8570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 750/750 [00:06<00:00, 111.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "Train Loss: 0.4278, Train Accuracy: 0.8478\n",
      "Val Loss: 0.3728, Val Accuracy: 0.8614, Val Macro-F1: 0.8600\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 750/750 [00:12<00:00, 59.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      "Train Loss: 0.4123, Train Accuracy: 0.8496\n",
      "Val Loss: 0.3562, Val Accuracy: 0.8673, Val Macro-F1: 0.8650\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 750/750 [00:14<00:00, 50.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      "Train Loss: 0.3943, Train Accuracy: 0.8575\n",
      "Val Loss: 0.3492, Val Accuracy: 0.8692, Val Macro-F1: 0.8678\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 750/750 [00:04<00:00, 155.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "Train Loss: 0.3858, Train Accuracy: 0.8602\n",
      "Val Loss: 0.3402, Val Accuracy: 0.8764, Val Macro-F1: 0.8751\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 750/750 [00:06<00:00, 109.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "Train Loss: 0.3769, Train Accuracy: 0.8636\n",
      "Val Loss: 0.3514, Val Accuracy: 0.8710, Val Macro-F1: 0.8680\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 750/750 [00:06<00:00, 116.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "Train Loss: 0.3699, Train Accuracy: 0.8655\n",
      "Val Loss: 0.3406, Val Accuracy: 0.8752, Val Macro-F1: 0.8747\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 750/750 [00:07<00:00, 105.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "Train Loss: 0.3584, Train Accuracy: 0.8692\n",
      "Val Loss: 0.3251, Val Accuracy: 0.8814, Val Macro-F1: 0.8815\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 10: 100%|██████████| 750/750 [00:05<00:00, 130.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10\n",
      "Train Loss: 0.3549, Train Accuracy: 0.8722\n",
      "Val Loss: 0.3328, Val Accuracy: 0.8804, Val Macro-F1: 0.8803\n",
      "\n",
      "[np.float64(0.5746926949775026), np.float64(0.4613905624695355), np.float64(0.4277792856682923), np.float64(0.4123108667140513), np.float64(0.39428277503210557), np.float64(0.38584172301478414), np.float64(0.3768922777271113), np.float64(0.36988420402859584), np.float64(0.35840337308198605), np.float64(0.3548978299264037)]\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "learning_rate = 0.005\n",
    "\n",
    "# Placeholder lists to store metrics\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1_scores = []\n",
    "\n",
    "# Function to create mini-batches\n",
    "def create_batches(X, y, batch_size):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, X.shape[0] - batch_size + 1, batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model_layers = model.layers\n",
    "    train_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # for X_batch, y_batch in create_batches(X_train, y_train, batch_size): \n",
    "    for X_batch, y_batch in tqdm(create_batches(X_train, y_train, batch_size), \n",
    "                                 desc=f\"Training Epoch {epoch+1}\", \n",
    "                                 total=int(np.ceil(X_train.shape[0] / batch_size))):\n",
    "        # Forward pass\n",
    "        logits = model.forward(X_batch, training=True)\n",
    "        loss = loss_function.forward(logits, y_batch)\n",
    "        train_loss += loss\n",
    "\n",
    "        # Backward pass\n",
    "        d_out = loss_function.backward(y_batch)\n",
    "        model.backward(d_out)\n",
    "\n",
    "        # Predictions and accuracy\n",
    "        predictions = np.argmax(loss_function.probabilities, axis=1)\n",
    "        correct_train += np.sum(predictions == y_batch)\n",
    "        total_train += y_batch.shape[0]\n",
    "        # print(correct_train, total_train)\n",
    "\n",
    "    avg_train_loss = train_loss / (X_train.shape[0] / batch_size)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    # Validation phase\n",
    "    val_loss = 0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    # No need to shuffle validation data\n",
    "    for X_batch, y_batch in create_batches(X_val, y_val, batch_size):\n",
    "        # Forward pass\n",
    "        logits = model.forward(X_batch, training=False)\n",
    "        loss = loss_function.forward(logits, y_batch)\n",
    "        val_loss += loss\n",
    "\n",
    "        # Predictions and accuracy\n",
    "        predictions = np.argmax(loss_function.probabilities, axis=1)\n",
    "        correct_val += np.sum(predictions == y_batch)\n",
    "        total_val += y_batch.shape[0]\n",
    "        all_predictions.extend(predictions)\n",
    "        all_targets.extend(y_batch)\n",
    "\n",
    "    avg_val_loss = val_loss / (X_val.shape[0] / batch_size)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Calculate macro-F1 score\n",
    "    macro_f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "    val_macro_f1_scores.append(macro_f1)\n",
    "\n",
    "    # Report metrics\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Macro-F1: {macro_f1:.4f}\\n\")\n",
    "\n",
    "print(train_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_27b5c table {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_27b5c th {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "#T_27b5c td {\n",
       "  border: 1px solid black;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_27b5c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_27b5c_level0_col0\" class=\"col_heading level0 col0\" >Train Loss</th>\n",
       "      <th id=\"T_27b5c_level0_col1\" class=\"col_heading level0 col1\" >Val Loss</th>\n",
       "      <th id=\"T_27b5c_level0_col2\" class=\"col_heading level0 col2\" >Train Accuracy</th>\n",
       "      <th id=\"T_27b5c_level0_col3\" class=\"col_heading level0 col3\" >Val Accuracy</th>\n",
       "      <th id=\"T_27b5c_level0_col4\" class=\"col_heading level0 col4\" >Val Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_27b5c_row0_col0\" class=\"data row0 col0\" >0.574693</td>\n",
       "      <td id=\"T_27b5c_row0_col1\" class=\"data row0 col1\" >0.409061</td>\n",
       "      <td id=\"T_27b5c_row0_col2\" class=\"data row0 col2\" >0.794083</td>\n",
       "      <td id=\"T_27b5c_row0_col3\" class=\"data row0 col3\" >0.850518</td>\n",
       "      <td id=\"T_27b5c_row0_col4\" class=\"data row0 col4\" >0.844064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_27b5c_row1_col0\" class=\"data row1 col0\" >0.461391</td>\n",
       "      <td id=\"T_27b5c_row1_col1\" class=\"data row1 col1\" >0.384785</td>\n",
       "      <td id=\"T_27b5c_row1_col2\" class=\"data row1 col2\" >0.833313</td>\n",
       "      <td id=\"T_27b5c_row1_col3\" class=\"data row1 col3\" >0.859208</td>\n",
       "      <td id=\"T_27b5c_row1_col4\" class=\"data row1 col4\" >0.857004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_27b5c_row2_col0\" class=\"data row2 col0\" >0.427779</td>\n",
       "      <td id=\"T_27b5c_row2_col1\" class=\"data row2 col1\" >0.372836</td>\n",
       "      <td id=\"T_27b5c_row2_col2\" class=\"data row2 col2\" >0.847833</td>\n",
       "      <td id=\"T_27b5c_row2_col3\" class=\"data row2 col3\" >0.861380</td>\n",
       "      <td id=\"T_27b5c_row2_col4\" class=\"data row2 col4\" >0.860000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_27b5c_row3_col0\" class=\"data row3 col0\" >0.412311</td>\n",
       "      <td id=\"T_27b5c_row3_col1\" class=\"data row3 col1\" >0.356236</td>\n",
       "      <td id=\"T_27b5c_row3_col2\" class=\"data row3 col2\" >0.849583</td>\n",
       "      <td id=\"T_27b5c_row3_col3\" class=\"data row3 col3\" >0.867313</td>\n",
       "      <td id=\"T_27b5c_row3_col4\" class=\"data row3 col4\" >0.864962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_27b5c_row4_col0\" class=\"data row4 col0\" >0.394283</td>\n",
       "      <td id=\"T_27b5c_row4_col1\" class=\"data row4 col1\" >0.349244</td>\n",
       "      <td id=\"T_27b5c_row4_col2\" class=\"data row4 col2\" >0.857500</td>\n",
       "      <td id=\"T_27b5c_row4_col3\" class=\"data row4 col3\" >0.869235</td>\n",
       "      <td id=\"T_27b5c_row4_col4\" class=\"data row4 col4\" >0.867840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_27b5c_row5_col0\" class=\"data row5 col0\" >0.385842</td>\n",
       "      <td id=\"T_27b5c_row5_col1\" class=\"data row5 col1\" >0.340150</td>\n",
       "      <td id=\"T_27b5c_row5_col2\" class=\"data row5 col2\" >0.860167</td>\n",
       "      <td id=\"T_27b5c_row5_col3\" class=\"data row5 col3\" >0.876420</td>\n",
       "      <td id=\"T_27b5c_row5_col4\" class=\"data row5 col4\" >0.875119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_27b5c_row6_col0\" class=\"data row6 col0\" >0.376892</td>\n",
       "      <td id=\"T_27b5c_row6_col1\" class=\"data row6 col1\" >0.351366</td>\n",
       "      <td id=\"T_27b5c_row6_col2\" class=\"data row6 col2\" >0.863646</td>\n",
       "      <td id=\"T_27b5c_row6_col3\" class=\"data row6 col3\" >0.870989</td>\n",
       "      <td id=\"T_27b5c_row6_col4\" class=\"data row6 col4\" >0.868032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_27b5c_row7_col0\" class=\"data row7 col0\" >0.369884</td>\n",
       "      <td id=\"T_27b5c_row7_col1\" class=\"data row7 col1\" >0.340639</td>\n",
       "      <td id=\"T_27b5c_row7_col2\" class=\"data row7 col2\" >0.865521</td>\n",
       "      <td id=\"T_27b5c_row7_col3\" class=\"data row7 col3\" >0.875167</td>\n",
       "      <td id=\"T_27b5c_row7_col4\" class=\"data row7 col4\" >0.874712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_27b5c_row8_col0\" class=\"data row8 col0\" >0.358403</td>\n",
       "      <td id=\"T_27b5c_row8_col1\" class=\"data row8 col1\" >0.325087</td>\n",
       "      <td id=\"T_27b5c_row8_col2\" class=\"data row8 col2\" >0.869167</td>\n",
       "      <td id=\"T_27b5c_row8_col3\" class=\"data row8 col3\" >0.881350</td>\n",
       "      <td id=\"T_27b5c_row8_col4\" class=\"data row8 col4\" >0.881459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_27b5c_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_27b5c_row9_col0\" class=\"data row9 col0\" >0.354898</td>\n",
       "      <td id=\"T_27b5c_row9_col1\" class=\"data row9 col1\" >0.332827</td>\n",
       "      <td id=\"T_27b5c_row9_col2\" class=\"data row9 col2\" >0.872188</td>\n",
       "      <td id=\"T_27b5c_row9_col3\" class=\"data row9 col3\" >0.880431</td>\n",
       "      <td id=\"T_27b5c_row9_col4\" class=\"data row9 col4\" >0.880274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2216503a990>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    'Train Loss': train_losses,\n",
    "    'Val Loss': val_losses,\n",
    "    'Train Accuracy': train_accuracies,\n",
    "    'Val Accuracy': val_accuracies,\n",
    "    'Val Macro F1': val_macro_f1_scores\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame(data)\n",
    "metrics_df = metrics_df.style.set_table_styles(\n",
    "    [{'selector': 'table', 'props': [('border', '1px solid black')]},\n",
    "     {'selector': 'th', 'props': [('border', '1px solid black')]},\n",
    "     {'selector': 'td', 'props': [('border', '1px solid black')]}]\n",
    ")\n",
    "# Display the table\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"1905105.pickle\", \"wb\") as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & Testing The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.36087644626330423\n",
      "Test Accuracy: 0.8729\n",
      "Test Macro-F1 Score: 0.8725546236397171\n",
      "Confusion Matrix:\n",
      " [[843   0   9  38   1   3 100   0   6   0]\n",
      " [  4 969   1  20   2   0   2   0   2   0]\n",
      " [ 14   1 807  15  73   0  87   0   3   0]\n",
      " [ 24   6   7 911  17   1  31   0   3   0]\n",
      " [  0   1 104  50 738   0 104   0   3   0]\n",
      " [  0   0   0   0   0 955   0  31   2  12]\n",
      " [142   1 103  35  51   1 653   0  14   0]\n",
      " [  0   0   0   0   0  21   0 931   0  48]\n",
      " [  2   1   2   4   3   4  15   3 966   0]\n",
      " [  0   0   0   0   0  14   1  29   0 956]]\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Function to evaluate the model on the test set\n",
    "def evaluate_model_on_test(model, X_test, y_test, loss_fn):\n",
    "    # Forward pass through the model to get logits\n",
    "    logits = model.forward(X_test, training=False)  # Set training=False to disable dropout\n",
    "    \n",
    "    # Calculate test loss\n",
    "    test_loss = loss_fn.forward(logits, y_test)     # Pass logits and targets to loss function\n",
    "    probabilities = loss_fn.probabilities\n",
    "    # Convert probabilities to class predictions\n",
    "    predicted_classes = np.argmax(probabilities, axis=1)  # Specify axis=1 to get predictions per sample\n",
    "    \n",
    "    # Calculate test accuracy and macro-F1 score\n",
    "    test_accuracy = accuracy_score(y_test, predicted_classes)\n",
    "    test_macro_f1 = f1_score(y_test, predicted_classes, average='macro')\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, predicted_classes)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    print(\"Test Macro-F1 Score:\", test_macro_f1)\n",
    "    print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "loss_fn = SoftmaxCrossEntropyLoss()\n",
    "with open(\"1905105.pickle\", \"rb\") as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "# loaded_model = model\n",
    "evaluate_model_on_test(loaded_model, X_test, y_test, loss_fn=loss_fn)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
