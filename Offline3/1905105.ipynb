{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import pickle\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01 # Taking random values and scaling them down to a smaller value\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Save the inputs for use in backpropagation\n",
    "        self.inputs = inputs\n",
    "        # Compute the linear output Z = XW + b\n",
    "        self.linear_output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.linear_output\n",
    "    \n",
    "    def backward(self, d_output, learning_rate=0.01):\n",
    "        # Calculate gradients for weights, biases, and inputs\n",
    "        self.d_weights = np.dot(self.inputs.T, d_output)\n",
    "        self.d_biases = np.sum(d_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Gradient for the inputs to pass to the previous layer\n",
    "        d_inputs = np.dot(d_output, self.weights.T)\n",
    "        \n",
    "        # Update weights and biases using the specified learning rate\n",
    "        self.weights -= learning_rate * self.d_weights\n",
    "        self.biases -= learning_rate * self.d_biases\n",
    "        \n",
    "        return d_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, output_size, momentum=0.9, epsilon=1e-8):\n",
    "        # Parameters for scaling and shifting\n",
    "        self.gamma = np.ones((1, output_size))  # Initialize scaling to 1\n",
    "        self.beta = np.zeros((1, output_size))  # Initialize shifting to 0\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        # Running mean and variance for inference (test time)\n",
    "        self.running_mean = np.zeros((1, output_size))\n",
    "        self.running_var = np.ones((1, output_size))\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            # Compute mean and variance for the current batch\n",
    "            self.batch_mean = np.mean(inputs, axis=0, keepdims=True)\n",
    "            self.batch_var = np.var(inputs, axis=0, keepdims=True)\n",
    "\n",
    "            # Normalize the inputs\n",
    "            self.x_normalized = (inputs - self.batch_mean) / np.sqrt(self.batch_var + self.epsilon)\n",
    "\n",
    "            # Scale and shift\n",
    "            self.out = self.gamma * self.x_normalized + self.beta\n",
    "\n",
    "            # Update running statistics for inference\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * self.batch_mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * self.batch_var\n",
    "        else:\n",
    "            # Use running mean and variance at test time\n",
    "            self.x_normalized = (inputs - self.running_mean) / np.sqrt(self.running_var + self.epsilon)\n",
    "            self.out = self.gamma * self.x_normalized + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, d_out, learning_rate=0.01):\n",
    "        # Backpropagation through batch normalization\n",
    "        batch_size = d_out.shape[0]\n",
    "\n",
    "        # Gradients for gamma and beta\n",
    "        self.d_gamma = np.sum(d_out * self.x_normalized, axis=0, keepdims=True)\n",
    "        self.d_beta = np.sum(d_out, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients for the normalized input\n",
    "        dx_normalized = d_out * self.gamma\n",
    "\n",
    "        # Gradients for variance\n",
    "        d_variance = np.sum(dx_normalized * (self.x_normalized * -0.5) * (self.batch_var + self.epsilon)**-1.5, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradients for mean\n",
    "        d_mean = np.sum(dx_normalized * -1 / np.sqrt(self.batch_var + self.epsilon), axis=0, keepdims=True) + d_variance * np.mean(-2 * (self.out - self.batch_mean), axis=0)\n",
    "\n",
    "        # Gradients for inputs\n",
    "        d_inputs = dx_normalized / np.sqrt(self.batch_var + self.epsilon) + (d_variance * 2 * (self.out - self.batch_mean) / batch_size) + (d_mean / batch_size)\n",
    "\n",
    "        # Update gamma and beta\n",
    "        self.gamma -= learning_rate * self.d_gamma\n",
    "        self.beta -= learning_rate * self.d_beta\n",
    "\n",
    "        return d_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, x):\n",
    "        # Save input for backward pass\n",
    "        self.x = x\n",
    "        # Apply ReLU\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Gradient of ReLU is 1 for positive x, 0 for negative x\n",
    "        d_input = d_output * (self.x > 0)\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def forward(self, inputs, training=True):\n",
    "        if training:\n",
    "            # Create a mask with the same shape as inputs, with `1` for neurons kept, `0` for dropped\n",
    "            self.mask = (np.random.rand(*inputs.shape) > self.dropout_rate).astype(float)\n",
    "            # Scale the mask to keep expected value consistent\n",
    "            return inputs * self.mask / (1 - self.dropout_rate)\n",
    "        else:\n",
    "            # During inference, scale down the output by (1 - dropout_rate)\n",
    "            return inputs\n",
    "\n",
    "    def backward(self, d_output):\n",
    "        # Apply the dropout mask to the gradient, ensuring only active neurons propagate gradients\n",
    "        return d_output * self.mask / (1 - self.dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = None  # First moment vector\n",
    "        self.v = None  # Second moment vector\n",
    "        self.t = 0     # Time step for bias correction\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        # Initialize m and v as zeros with the same shape as parameters\n",
    "        if self.m is None:\n",
    "            self.m = [np.zeros_like(param) for param in params]\n",
    "            self.v = [np.zeros_like(param) for param in params]\n",
    "\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "\n",
    "        # Update parameters\n",
    "        for i in range(len(params)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grads[i]\n",
    "            # Update biased second moment estimate\n",
    "            self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grads[i] ** 2)\n",
    "\n",
    "            # Correct bias in first moment\n",
    "            m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
    "            # Correct bias in second moment\n",
    "            v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "            # Update parameters\n",
    "            params[i] -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # stability trick\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(predictions, labels):\n",
    "    one_hot_labels = np.zeros_like(predictions)\n",
    "    one_hot_labels[np.arange(len(labels)), labels] = 1\n",
    "    probs = softmax(predictions)\n",
    "    loss = -np.mean(np.sum(one_hot_labels * np.log(probs + 1e-8), axis=1))  # Add epsilon to avoid log(0)\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_gradient(predictions, labels):\n",
    "    one_hot_labels = np.zeros_like(predictions)\n",
    "    one_hot_labels[np.arange(len(labels)), labels] = 1\n",
    "    probs = softmax(predictions)\n",
    "    return (probs - one_hot_labels) / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            DenseLayer(28*28, 128),\n",
    "            BatchNormalization(128),\n",
    "            ReLU(),\n",
    "            Dropout(),\n",
    "            DenseLayer(128, 32),\n",
    "            BatchNormalization(32),\n",
    "            ReLU(),\n",
    "            Dropout(),\n",
    "            DenseLayer(32, 10)\n",
    "        ]\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X):\n",
    "        logits = self.forward(X)\n",
    "        probabilities = softmax(logits)\n",
    "        return np.argmax(probabilities, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, train_loader, epochs=10, learning_rate=0.001):\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            # Flatten images and prepare labels\n",
    "            images = images.view(-1, 28*28).numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = network.forward(images)\n",
    "            # loss = cross_entropy_loss(predictions, labels)\n",
    "            # losses.append(loss)\n",
    "\n",
    "            # Backward pass\n",
    "            grad_output = cross_entropy_gradient(predictions, labels)\n",
    "            for layer in reversed(network.layers):\n",
    "                if isinstance(layer, DenseLayer):\n",
    "                    grad_output = layer.backward(grad_output, learning_rate)\n",
    "                else:\n",
    "                    grad_output = layer.backward(grad_output)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {np.mean(losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Instantiate softmax, cross-entropy loss, and metrics tracking\n",
    "softmax = Softmax()\n",
    "cross_entropy_loss = CrossEntropyLoss()\n",
    "\n",
    "# Placeholder lists to store metrics over epochs\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "val_macro_f1_scores = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss, correct_train, total_train = 0, 0, 0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        logits = model.forward(inputs)        # Pass inputs through the network\n",
    "        predictions = softmax.forward(logits) # Apply softmax to get probabilities\n",
    "        \n",
    "        # Compute loss and accumulate it\n",
    "        loss = cross_entropy_loss.forward(predictions, targets)\n",
    "        train_loss += loss\n",
    "        \n",
    "        # Backward pass\n",
    "        d_loss = cross_entropy_loss.backward(predictions, targets)\n",
    "        model.backward(d_loss)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        correct_train += (predicted_classes == targets).sum()\n",
    "        total_train += targets.shape[0]\n",
    "    \n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_losses.append(avg_train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss, correct_val, total_val = 0, 0, 0\n",
    "    all_pred, all_true = [], []\n",
    "    \n",
    "    for inputs, targets in val_loader:\n",
    "        # Forward pass\n",
    "        logits = model.forward(inputs)\n",
    "        predictions = softmax.forward(logits)\n",
    "        \n",
    "        # Compute loss and accumulate it\n",
    "        loss = cross_entropy_loss.forward(predictions, targets)\n",
    "        val_loss += loss\n",
    "        \n",
    "        # Accuracy calculations\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        correct_val += (predicted_classes == targets).sum()\n",
    "        total_val += targets.shape[0]\n",
    "        \n",
    "        # Store predictions and targets for F1 score calculation\n",
    "        all_pred.extend(predicted_classes)\n",
    "        all_true.extend(targets)\n",
    "    \n",
    "    # Calculate average validation loss, accuracy, and macro F1 score\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = correct_val / total_val\n",
    "    macro_f1 = f1_score(all_true, all_pred, average='macro')\n",
    "    \n",
    "    val_losses.append(avg_val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    val_macro_f1_scores.append(macro_f1)\n",
    "    \n",
    "    # Print metrics for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val Macro-F1: {macro_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 67.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 70.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 70.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 68.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:13<00:00, 67.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 65.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 65.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 65.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 65.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:14<00:00, 62.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform and load dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize and train the network\n",
    "network = NeuralNetwork()\n",
    "train(network, train_loader, epochs=10, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, data_loader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images = images.view(-1, 28*28).numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        # Get predictions\n",
    "        predicted_labels = network.predict(images)\n",
    "        correct_predictions += np.sum(predicted_labels == labels)\n",
    "        total_predictions += len(labels)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1416\n"
     ]
    }
   ],
   "source": [
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "evaluate(network, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
