{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform and load dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data = datasets.FashionMNIST(root=\"data\", train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        # print(\"YEs in Dense Layer\")\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def backward(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights and biases\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "        \n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, size, momentum=0.9):\n",
    "        self.gamma = np.ones((1, size))\n",
    "        self.beta = np.zeros((1, size))\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = np.zeros((1, size))\n",
    "        self.running_var = np.ones((1, size))\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if training:\n",
    "            mean = np.mean(X, axis=0, keepdims=True)\n",
    "            var = np.var(X, axis=0, keepdims=True)\n",
    "            X_norm = (X - mean) / np.sqrt(var + 1e-8)\n",
    "            self.running_mean = self.momentum * self.running_mean + (1 - self.momentum) * mean\n",
    "            self.running_var = self.momentum * self.running_var + (1 - self.momentum) * var\n",
    "        else:\n",
    "            X_norm = (X - self.running_mean) / np.sqrt(self.running_var + 1e-8)\n",
    "        # print(\"YEs in BN\")\n",
    "        return self.gamma * X_norm + self.beta\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return grad_output  # Stub, we can refine later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, X):\n",
    "        self.input = X\n",
    "        # print(\"YEs in ReLU\")\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output * (self.input > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(logits):\n",
    "    exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # stability trick\n",
    "    return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(predictions, labels):\n",
    "    one_hot_labels = np.zeros_like(predictions)\n",
    "    one_hot_labels[np.arange(len(labels)), labels] = 1\n",
    "    probs = softmax(predictions)\n",
    "    loss = -np.mean(np.sum(one_hot_labels * np.log(probs + 1e-8), axis=1))  # Add epsilon to avoid log(0)\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_gradient(predictions, labels):\n",
    "    one_hot_labels = np.zeros_like(predictions)\n",
    "    one_hot_labels[np.arange(len(labels)), labels] = 1\n",
    "    probs = softmax(predictions)\n",
    "    return (probs - one_hot_labels) / len(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = [\n",
    "            DenseLayer(28*28, 128),\n",
    "            BatchNormalization(128),\n",
    "            ReLU(),\n",
    "            DenseLayer(128, 10)  # Output layer with 10 classes for FashionMNIST\n",
    "        ]\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        logits = self.forward(X)\n",
    "        probabilities = softmax(logits)\n",
    "        return np.argmax(probabilities, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, train_loader, epochs=10, learning_rate=0.001):\n",
    "    for epoch in range(epochs):\n",
    "        losses = []\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images = images.view(-1, 28*28).numpy()\n",
    "            labels = labels.numpy()\n",
    "\n",
    "            predictions = network.forward(images)\n",
    "            loss = cross_entropy_loss(predictions, labels)\n",
    "            losses.append(loss)\n",
    "\n",
    "            grad_output = cross_entropy_gradient(predictions, labels)\n",
    "            for layer in reversed(network.layers):\n",
    "                if isinstance(layer, DenseLayer):\n",
    "                    grad_output = layer.backward(grad_output, learning_rate)\n",
    "                else:\n",
    "                    grad_output = layer.backward(grad_output)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {np.mean(losses)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/938 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:24<00:00, 38.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7526639840706704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:19<00:00, 49.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.2691464567588226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:20<00:00, 46.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.1020526005439706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:19<00:00, 48.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 0.9979376054811355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 49.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 0.9313366874504296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.8820911275757519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:21<00:00, 43.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.844588215601471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:19<00:00, 48.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.8171254780712017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 50.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.7930072273094022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 938/938 [00:18<00:00, 51.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.7728278619664192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the network\n",
    "network = NeuralNetwork()\n",
    "train(network, train_loader, epochs=10, learning_rate=0.001)\n",
    "\n",
    "# Save the model\n",
    "with open(\"1905105.pickle\", \"wb\") as f:\n",
    "    pickle.dump(network, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, data_loader):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images = images.view(-1, 28*28).numpy()\n",
    "        labels = labels.numpy()\n",
    "\n",
    "        # Get predictions\n",
    "        predicted_labels = network.predict(images)\n",
    "        correct_predictions += np.sum(predicted_labels == labels)\n",
    "        total_predictions += len(labels)\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7356\n"
     ]
    }
   ],
   "source": [
    "# After training, evaluate on a test set\n",
    "test_data = datasets.FashionMNIST(root=\"data\", train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "evaluate(network, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
